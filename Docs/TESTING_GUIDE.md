# ğŸ§ª AI Integration Testing Guide

**Quick Start:** Get your AI-powered study companion running in 5 minutes!

---

## ğŸ“‹ Pre-Testing Checklist

Before testing, ensure you have:

- âœ… OpenAI API key ready
- âœ… Bun runtime installed
- âœ… All dependencies installed (`bun install`)
- âœ… Development environment ready

---

## ğŸ”‘ Step 1: Add Your OpenAI API Key

### Option A: Create `.env.local` file (Recommended)

```bash
# In your project root
touch .env.local

# Add your key (replace with your actual key)
echo "OPENAI_API_KEY=sk-..." >> .env.local
```

### Option B: Copy from example

```bash
# If .env.example exists
cp .env.example .env.local

# Then edit .env.local and add your key
```

### Your `.env.local` should look like:

```env
# OpenAI Configuration
OPENAI_API_KEY=sk-proj-your-actual-key-here
OPENAI_MODEL=gpt-4
OPENAI_MAX_TOKENS=500
OPENAI_TEMPERATURE=0.8
OPENAI_CONTEXT_WINDOW_SIZE=15
OPENAI_LOG_USAGE=true
```

**âš ï¸ Important:** Never commit `.env.local` to git! It's already in `.gitignore`.

---

## âœ… Step 2: Verify OpenAI Connection

Run the verification script:

```bash
bun run verify-openai
```

### Expected Output:

```
ğŸ” OpenAI API Connection Verification
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… Configuration Check
   Model: gpt-4
   Max Tokens: 500
   Temperature: 0.8

âœ… API Key Format
   Valid format detected

âœ… Connection Test
   Successfully connected to OpenAI API
   Response: Hello! I'm working perfectly...

âœ… Token Usage Test
   Prompt tokens: 25
   Completion tokens: 15
   Total tokens: 40
   Estimated cost: $0.0012

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… All checks passed!
```

### Troubleshooting:

If you see errors:

**âŒ "Invalid API key"**

- Check your key is correct
- Ensure no extra spaces
- Verify key starts with `sk-`

**âŒ "Rate limit exceeded"**

- Wait a few seconds and try again
- Check your OpenAI usage limits

**âŒ "Network error"**

- Check your internet connection
- Verify no firewall blocking OpenAI

---

## ğŸš€ Step 3: Start the Development Server

```bash
bun run dev
```

Navigate to: `http://localhost:3000`

---

## ğŸ§ª Testing Scenarios

### Test 1: Basic AI Chat (Age-Appropriate Responses)

**Test with Lucas (Age 9):**

1. Login as Lucas
2. Ask: "Can you help me with fractions?"
3. **Expected:** Simple language, emojis, encouraging tone

**Test with Eva (Age 12):**

1. Login as Eva
2. Ask: "What's a metaphor?"
3. **Expected:** Balanced tone, some complexity, relatable examples

**Test with Pat (Age 16):**

1. Login as Pat
2. Ask: "Explain photosynthesis"
3. **Expected:** Academic language, detailed explanation, critical thinking prompts

### Test 2: Multiple Choice Task Generation

```bash
# In your app, request a practice task for Math/Fractions
```

**Expected Task Structure:**

- âœ… Question about fractions
- âœ… Exactly 4 options (A, B, C, D)
- âœ… One correct answer
- âœ… 3 plausible distractors
- âœ… Explanation included
- âœ… 3 progressive hints
- âœ… Age-appropriate language

**Test grading:**

- Submit correct answer â†’ "ğŸ‰ Excellent work!"
- Submit incorrect answer â†’ Encouraging feedback + hint offer

### Test 3: Open-Ended Task Generation

**Request an open-ended question:**

**Expected:**

- âœ… Thought-provoking question
- âœ… Rubric with 3-5 key points
- âœ… Word count guidance
- âœ… 3 hints
- âœ… Sample answer (optional)

**Test AI Grading:**

- Submit a good answer â†’ 70-100 score + positive feedback
- Submit a weak answer â†’ 30-60 score + constructive suggestions
- Submit off-topic â†’ Low score + redirection

### Test 4: Real-World Task Generation

**Request a hands-on activity:**

**Expected:**

- âœ… Engaging activity title
- âœ… 3-5 clear instructions
- âœ… Materials list (household items)
- âœ… 3 verification questions
- âœ… Reflection prompt
- âœ… Safety notes (if needed)

### Test 5: Adaptive Task Assignment

```typescript
// This happens automatically when student requests tasks
// System should:
```

**Expected Behavior:**

- âœ… Identifies topics with lowest progress
- âœ… Assigns appropriate difficulty (easy/medium/hard)
- âœ… Mixes task types based on age:
  - Age 9-11: More MC, some OE, less RW
  - Age 12-14: Balanced MC and OE
  - Age 15-16: More OE, less MC
- âœ… Limits to 5 tasks maximum
- âœ… Prioritizes struggling topics

### Test 6: Struggle Detection

**Simulate struggling:**

1. Get 3 tasks wrong in a row
2. Use phrases like "I'm confused" or "I don't get it"
3. Ask for help multiple times

**Expected Interventions:**

- After 2 failures â†’ "Let's try an easier problem!"
- After 3 failures â†’ "How about we schedule time with a tutor?"
- Frustration detected â†’ "Would you like a hint?"

### Test 7: Tutor Handoff Notes

**After simulating struggles:**

```typescript
// Generated automatically when tutor is suggested
// Or can be triggered manually for testing
```

**Expected Notes Should Include:**

- âœ… Brief summary (2-3 sentences)
- âœ… Specific struggling topics
- âœ… Teaching recommendations
- âœ… Emotional state assessment
- âœ… Formatted as clear bullet points

---

## ğŸ“Š Monitoring & Logs

### Check Token Usage:

Look in console output for:

```
[TOKEN USAGE] Operation: chat_response
  Model: gpt-4
  Prompt tokens: 450
  Completion tokens: 120
  Total tokens: 570
  Cost: $0.017
  Latency: 1850ms
  Success: true
```

### Verify Features:

- âœ… Age-appropriate tone works
- âœ… Tasks generate correctly
- âœ… Grading is accurate
- âœ… Difficulty adapts to progress
- âœ… Struggle detection triggers
- âœ… Handoff notes are helpful

---

## ğŸ› Common Issues & Solutions

### Issue: Tasks not generating

**Solution:**

- Check OpenAI API key is valid
- Verify network connection
- Check console for error messages
- Retry with exponential backoff (automatic)

### Issue: Responses too slow

**Causes:**

- Large context window
- Complex prompts
- Network latency

**Solutions:**

- Reduce `OPENAI_CONTEXT_WINDOW_SIZE` to 10
- Use `gpt-3.5-turbo` for faster responses (less accurate)
- Implement caching (Phase 8)

### Issue: Token costs too high

**Monitor:**

- Check usage logs in console
- Track daily spending in OpenAI dashboard

**Optimize:**

- Reduce max_tokens (currently 500-1200)
- Shorten system prompts
- Implement caching
- Use summarization more aggressively

### Issue: Grading inconsistent

**Notes:**

- OpenAI responses have some randomness (temperature)
- Lower temperature (0.3) for grading gives more consistency
- Test same answer multiple times to verify

---

## ğŸ“ˆ Performance Benchmarks

### Target Metrics:

| Operation       | Target Time | Target Cost  |
| --------------- | ----------- | ------------ |
| Chat Response   | < 3s        | $0.01-0.02   |
| Task Generation | < 4s        | $0.02-0.03   |
| Task Grading    | < 2s        | $0.01-0.02   |
| Handoff Notes   | < 3s        | $0.015-0.025 |

### Daily Cost Estimate:

**Per Student (10 interactions/day):**

- 5 chat messages: ~$0.08
- 3 task generations: ~$0.07
- 2 task gradings: ~$0.03
- **Total: ~$0.18/student/day**

**100 Students:**

- $18/day
- $540/month
- $6,480/year

_Using GPT-4. GPT-3.5-turbo is 10x cheaper but less accurate._

---

## ğŸ¯ Feature Checklist

After testing, verify:

### Core Features:

- [ ] OpenAI connection works
- [ ] Age-appropriate responses (test all 3 ages)
- [ ] Multiple choice tasks generate correctly
- [ ] Open-ended tasks generate correctly
- [ ] Real-world activities generate correctly
- [ ] Instant MC grading works
- [ ] AI grading for open-ended works
- [ ] Adaptive difficulty works
- [ ] Task type distribution correct by age
- [ ] Struggle detection triggers appropriately
- [ ] Interventions are helpful
- [ ] Handoff notes are comprehensive

### Technical Features:

- [ ] Error handling works (test with invalid key)
- [ ] Retry logic works (test with network issues)
- [ ] Token usage logged correctly
- [ ] Cost estimates accurate
- [ ] Context window management works
- [ ] Conversation summarization works
- [ ] No TypeScript errors
- [ ] No console errors (except expected logs)

---

## ğŸ“ Student Personas for Testing

Use these test personas:

### Lucas (Age 9, Grade 4)

- **Subjects:** Math (Fractions, Division), Science (Animals)
- **Expected Tone:** Simple, emojis, very encouraging
- **Task Types:** 50% MC, 30% OE, 20% RW
- **Topics:** Fractions (30%), Division (25%), Decimals (20%)

### Eva (Age 12, Grade 7)

- **Subjects:** English (Metaphors), Math (Algebra)
- **Expected Tone:** Balanced, some complexity
- **Task Types:** 40% MC, 40% OE, 20% RW
- **Topics:** Metaphors (struggling), Algebra, Writing

### Pat (Age 16, Grade 11)

- **Subjects:** Math (Calculus), Science (Physics)
- **Expected Tone:** Academic, challenging
- **Task Types:** 30% MC, 50% OE, 20% RW
- **Topics:** Derivatives, Momentum, Stoichiometry

### Mia (Age 10, Grade 5)

- **Subjects:** Math (Fractions), English (Grammar)
- **Expected Tone:** Simple but not babyish
- **Task Types:** 50% MC, 30% OE, 20% RW
- **Topics:** Various, well-rounded

---

## ğŸ“ Test Results Template

Copy this to document your testing:

```markdown
## Test Results - [Date]

### Environment:

- Node/Bun Version:
- OpenAI Model: gpt-4
- API Key Status: âœ… Valid

### Test 1: Basic Chat

- Lucas (Age 9): âœ… Pass / âŒ Fail
  - Notes:
- Eva (Age 12): âœ… Pass / âŒ Fail
  - Notes:
- Pat (Age 16): âœ… Pass / âŒ Fail
  - Notes:

### Test 2: Task Generation

- Multiple Choice: âœ… Pass / âŒ Fail
  - Quality: [1-5]
  - Time: [seconds]
  - Notes:
- Open-Ended: âœ… Pass / âŒ Fail
  - Quality: [1-5]
  - Time: [seconds]
  - Notes:
- Real-World: âœ… Pass / âŒ Fail
  - Quality: [1-5]
  - Time: [seconds]
  - Notes:

### Test 3: Grading

- MC Grading: âœ… Pass / âŒ Fail
- OE Grading: âœ… Pass / âŒ Fail
- Feedback Quality: [1-5]

### Test 4: Adaptive Features

- Difficulty Adaptation: âœ… Pass / âŒ Fail
- Task Type Distribution: âœ… Pass / âŒ Fail
- Topic Prioritization: âœ… Pass / âŒ Fail

### Test 5: Struggle Detection

- Task Failure Detection: âœ… Pass / âŒ Fail
- Conversation Monitoring: âœ… Pass / âŒ Fail
- Interventions: âœ… Pass / âŒ Fail
- Handoff Notes: âœ… Pass / âŒ Fail

### Performance:

- Average Response Time: [seconds]
- Total Tokens Used: [count]
- Estimated Cost: $[amount]

### Issues Found:

1.
2.
3.

### Overall Assessment:

- System Status: âœ… Ready / âš ï¸ Needs Work / âŒ Major Issues
- Recommendation:
```

---

## ğŸš€ Next Steps After Testing

### If Everything Works:

1. âœ… Document any configuration changes
2. âœ… Note optimal settings for your use case
3. âœ… Plan for production deployment
4. âœ… Set up monitoring/alerts
5. âœ… Consider Phase 8 optimizations (caching, batching)

### If Issues Found:

1. ğŸ› Document all issues clearly
2. ğŸ“ Note reproduction steps
3. ğŸ” Check logs and error messages
4. ğŸ’¬ Share findings for troubleshooting

---

## ğŸ’¡ Tips for Best Results

1. **Start Simple:** Test basic chat first, then move to complex features
2. **One Feature at a Time:** Don't test everything at once
3. **Monitor Costs:** Keep OpenAI dashboard open during testing
4. **Use Test Mode:** Consider OpenAI's test environment for initial testing
5. **Document Everything:** Save examples of good and bad outputs
6. **Test Edge Cases:** Try unusual inputs, long conversations, etc.
7. **Performance First:** If slow, optimize before testing everything
8. **Iterate:** Testing â†’ Fix â†’ Re-test cycle

---

## ğŸ“ Support

**Issues with this guide?**

- Check `Docs/ENV_SETUP.md` for environment details
- Review `Docs/PRD5/AITasks.md` for implementation details
- Check console logs for detailed error messages

**OpenAI API Issues?**

- Visit: https://platform.openai.com/docs
- Check status: https://status.openai.com
- View usage: https://platform.openai.com/usage

---

Good luck with testing! ğŸ‰

Remember: This is a development environment. Test thoroughly before production use!
